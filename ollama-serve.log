time=2024-04-08T12:57:04.859Z level=INFO source=images.go:804 msg="total blobs: 46"
time=2024-04-08T12:57:04.867Z level=INFO source=images.go:811 msg="total unused blobs removed: 0"
time=2024-04-08T12:57:04.871Z level=INFO source=routes.go:1118 msg="Listening on 127.0.0.1:11434 (version 0.1.30)"
time=2024-04-08T12:57:04.871Z level=INFO source=payload_common.go:113 msg="Extracting dynamic libraries to /tmp/ollama1293111853/runners ..."
time=2024-04-08T12:57:07.616Z level=INFO source=payload_common.go:140 msg="Dynamic LLM libraries [cpu_avx rocm_v60000 cpu cpu_avx2 cuda_v11]"
time=2024-04-08T12:57:07.617Z level=INFO source=gpu.go:115 msg="Detecting GPU type"
time=2024-04-08T12:57:07.617Z level=INFO source=gpu.go:265 msg="Searching for GPU management library libcudart.so*"
time=2024-04-08T12:57:07.619Z level=INFO source=gpu.go:311 msg="Discovered GPU libraries: [/tmp/ollama1293111853/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.1.105]"
time=2024-04-08T12:57:07.658Z level=INFO source=gpu.go:120 msg="Nvidia GPU detected via cudart"
time=2024-04-08T12:57:07.658Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T12:57:07.737Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
[GIN] 2024/04/08 - 12:57:14 | 200 |      45.474µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/04/08 - 12:57:15 | 200 |   17.390952ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:00:21 | 200 |      32.581µs |       127.0.0.1 | GET      "/"
[GIN] 2024/04/08 - 13:49:27 | 200 |    5.257106ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:49:27 | 200 |    3.622753ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:49:28 | 200 |    4.921651ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:49:29 | 200 |      33.802µs |       127.0.0.1 | GET      "/api/version"
time=2024-04-08T13:49:52.012Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T13:49:52.012Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T13:49:52.013Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T13:49:52.013Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T13:49:52.013Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T13:49:52.017Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1293111853/runners/cuda_v11/libext_server.so"
time=2024-04-08T13:49:52.017Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 23 key-value pairs and 322 tensors from /teamspace/studios/this_studio/.ollama/models/blobs/sha256-8a9611e7bca168be635d39d21927d2b8e7e8ea0b5d0998b7d5980daf1f8d4205 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = command-r
llama_model_loader: - kv   1:                               general.name str              = c4ai-command-r-v01
llama_model_loader: - kv   2:                      command-r.block_count u32              = 40
llama_model_loader: - kv   3:                   command-r.context_length u32              = 131072
llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192
llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528
llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64
llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64
llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500
llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,256000]  = ["<PAD>", "<UNK>", "<CLS>", "<SEP>", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,253333]  = ["Ġ Ġ", "Ġ t", "e r", "i n", "Ġ a...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 5
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 255001
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   41 tensors
llama_model_loader: - type q4_0:  280 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 1008/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = command-r
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 253333
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 64
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 8192
llm_load_print_meta: n_embd_v_gqa     = 8192
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 6.2e-02
llm_load_print_meta: n_ff             = 22528
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = none
llm_load_print_meta: freq_base_train  = 8000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 35B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 34.98 B
llm_load_print_meta: model size       = 18.83 GiB (4.62 BPW) 
llm_load_print_meta: general.name     = c4ai-command-r-v01
llm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'
llm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'
llm_load_print_meta: PAD token        = 0 '<PAD>'
llm_load_print_meta: LF token         = 136 'Ä'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes
ggml_cuda_init: CUDA_USE_TENSOR_CORES: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes
llm_load_tensors: ggml ctx size =    0.25 MiB
llm_load_tensors: offloading 37 repeating layers to GPU
llm_load_tensors: offloaded 37/41 layers to GPU
llm_load_tensors:        CPU buffer size = 19281.91 MiB
llm_load_tensors:      CUDA0 buffer size = 16318.16 MiB
.................................................................[GIN] 2024/04/08 - 13:51:18 | 200 |   49.167523ms |       127.0.0.1 | GET      "/api/tags"
....[GIN] 2024/04/08 - 13:51:19 | 200 |    5.542584ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:51:19 | 200 |     796.947µs |       127.0.0.1 | GET      "/api/version"
..................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 8000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   192.00 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =  2368.00 MiB
llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =   516.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  2172.62 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    36.00 MiB
llama_new_context_with_model: graph nodes  = 1245
llama_new_context_with_model: graph splits = 34
loading library /tmp/ollama1293111853/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"140547685807872","timestamp":1712584285}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"140547685807872","timestamp":1712584285}
time=2024-04-08T13:51:25.624Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"140546865133312","timestamp":1712584285}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"140546865133312","timestamp":1712584285}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":8,"slot_id":0,"task_id":0,"tid":"140546865133312","timestamp":1712584285}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"140546865133312","timestamp":1712584285}
[GIN] 2024/04/08 - 13:51:32 | 200 |         1m41s |       127.0.0.1 | POST     "/api/chat"
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":31,"n_ctx":2048,"n_past":30,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"140546865133312","timestamp":1712584292,"truncated":false}
[GIN] 2024/04/08 - 13:51:35 | 200 |    4.977868ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:51:36 | 200 |    4.786747ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:51:36 | 200 |      25.671µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 13:51:55 | 200 |    5.377184ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 13:52:01 | 200 |      72.961µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 13:52:30 | 200 |      36.421µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 13:52:39 | 200 |     4.32189ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:00:25 | 200 |      5.2689ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:00:27 | 200 |    7.551916ms |       127.0.0.1 | GET      "/api/tags"
time=2024-04-08T14:00:45.466Z level=WARN source=parser.go:73 msg="Unknown command: SYSTEM\nSimulate"
time=2024-04-08T14:00:47.447Z level=INFO source=download.go:136 msg="downloading 8934d96d3f08 in 39 100 MB part(s)"
time=2024-04-08T14:01:00.357Z level=INFO source=download.go:136 msg="downloading 8c17c2ebb0ea in 1 7.0 KB part(s)"
time=2024-04-08T14:01:02.115Z level=INFO source=download.go:136 msg="downloading 7c23fb36d801 in 1 4.8 KB part(s)"
time=2024-04-08T14:01:04.948Z level=INFO source=download.go:136 msg="downloading 2e0493f67d0c in 1 59 B part(s)"
time=2024-04-08T14:01:06.688Z level=INFO source=download.go:136 msg="downloading fa304d675061 in 1 91 B part(s)"
time=2024-04-08T14:01:08.482Z level=INFO source=download.go:136 msg="downloading 42ba7f8a01dd in 1 557 B part(s)"
[GIN] 2024/04/08 - 14:01:15 | 200 |   22.120241ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:01:16 | 200 |    6.815744ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:01:23 | 200 | 37.649310403s |       127.0.0.1 | POST     "/api/create"
[GIN] 2024/04/08 - 14:02:29 | 200 |    6.502925ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:02:30 | 200 |    4.889684ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:04 | 200 |    5.458903ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:05 | 200 |    4.703318ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:18 | 200 |    5.932551ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:19 | 200 |    5.361888ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:34 | 200 |    5.423782ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:36 | 200 |    5.093712ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:42 | 200 |    4.994808ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:43 | 200 |    5.648547ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:10:45 | 200 |      38.092µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 14:12:12 | 200 |    5.426437ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:12:13 | 200 |     4.63048ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:14:50 | 200 |      38.413µs |       127.0.0.1 | GET      "/api/version"
time=2024-04-08T14:15:25.998Z level=WARN source=parser.go:73 msg="Unknown command: SYSTEM\n你是犀照科技的AI小助手。\n用户询问到物料相关的知识时，你可以从知识库中寻找资料，结合你的其他知识给出综合的答案。\n如果没有知识库，或知识库没有答案，或者用户询问物料无关的知识时，可以引导用户提供更详细的信息。\n如果材料有相关的链接，要提供给用户。\n"
[GIN] 2024/04/08 - 14:15:26 | 200 |     8.22739ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2024/04/08 - 14:15:26 | 200 |    6.313682ms |       127.0.0.1 | GET      "/api/tags"
time=2024-04-08T14:15:38.596Z level=WARN source=parser.go:73 msg="Unknown command: SYSTEM\n你是犀照科技的AI小助手。\n用户询问到物料相关的知识时，你可以从知识库中寻找资料，结合你的其他知识给出综合的答案。\n如果没有知识库，或知识库没有答案，或者用户询问物料无关的知识时，可以引导用户提供更详细的信息。\n如果材料有相关的链接，要提供给用户。\n"
[GIN] 2024/04/08 - 14:15:38 | 200 |    7.414821ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2024/04/08 - 14:15:39 | 200 |    5.287708ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:15:54 | 200 |    5.994893ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:15:56 | 200 |    4.908845ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:15:56 | 200 |      32.572µs |       127.0.0.1 | GET      "/api/version"
time=2024-04-08T14:17:14.256Z level=WARN source=parser.go:73 msg="Unknown command: SYSTEM\n你是犀照科技的AI小助手。\n用户询问到物料相关的知识时，你可以从知识库中寻找资料，结合你的其他知识给出综合的答案。\n如果没有知识库，或知识库没有答案，或者用户询问物料无关的知识时，可以引导用户提供更详细的信息。\n如果材料有相关的链接，要提供给用户。\n"
[GIN] 2024/04/08 - 14:17:14 | 200 |    8.184023ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2024/04/08 - 14:17:14 | 200 |     4.79431ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:17:17 | 200 |    5.038085ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:17:19 | 200 |    5.015485ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/04/08 - 14:17:19 | 200 |      32.472µs |       127.0.0.1 | GET      "/api/version"
time=2024-04-08T14:17:21.959Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:17:21.964Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T14:17:21.964Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:17:21.964Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T14:17:21.965Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:17:21.970Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1293111853/runners/cuda_v11/libext_server.so"
time=2024-04-08T14:17:21.970Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /teamspace/studios/this_studio/.ollama/models/blobs/sha256-353e0b65ea8f927573a06b3c6748c5e27bf6cf35fcdca9f301ed64611d1017f9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 5000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,64000]   = ["<unk>", "<s>", "</s>", "<|Human|>",...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,64000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,64000]   = [2, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs 267/64000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 64000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 5000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.06 B
llm_load_print_meta: model size       = 3.24 GiB (4.59 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 315 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   140.62 MiB
llm_load_tensors:      CUDA0 buffer size =  3176.10 MiB
............................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 5000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   128.00 MiB
llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =   133.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1293111853/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"140544122537728","timestamp":1712585852}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"140544122537728","timestamp":1712585852}
time=2024-04-08T14:17:32.167Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"140543921174272","timestamp":1712585852}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712585852}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":30,"slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712585852}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712585852}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      73.52 ms /    30 tokens (    2.45 ms per token,   408.07 tokens per second)","n_prompt_tokens_processed":30,"n_tokens_second":408.0744327765384,"slot_id":0,"t_prompt_processing":73.516,"t_token":2.4505333333333335,"task_id":0,"tid":"140543921174272","timestamp":1712585853}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1649.92 ms /   100 runs   (   16.50 ms per token,    60.61 tokens per second)","n_decoded":100,"n_tokens_second":60.60892575527813,"slot_id":0,"t_token":16.49922,"t_token_generation":1649.922,"task_id":0,"tid":"140543921174272","timestamp":1712585853}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1723.44 ms","slot_id":0,"t_prompt_processing":73.516,"t_token_generation":1649.922,"t_total":1723.438,"task_id":0,"tid":"140543921174272","timestamp":1712585853}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":130,"n_ctx":2048,"n_past":129,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712585853,"truncated":false}
[GIN] 2024/04/08 - 14:17:33 | 200 |  12.14507367s |       127.0.0.1 | POST     "/api/chat"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":3,"n_past_se":0,"n_prompt_tokens_processed":61,"slot_id":0,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":3,"slot_id":0,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      74.66 ms /    61 tokens (    1.22 ms per token,   817.07 tokens per second)","n_prompt_tokens_processed":61,"n_tokens_second":817.0700671069023,"slot_id":0,"t_prompt_processing":74.657,"t_token":1.2238852459016394,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     131.40 ms /     9 runs   (   14.60 ms per token,    68.49 tokens per second)","n_decoded":9,"n_tokens_second":68.4931506849315,"slot_id":0,"t_token":14.600000000000001,"t_token_generation":131.4,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     206.06 ms","slot_id":0,"t_prompt_processing":74.657,"t_token_generation":131.4,"t_total":206.05700000000002,"task_id":103,"tid":"140543921174272","timestamp":1712585854}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":73,"n_ctx":2048,"n_past":72,"n_system_tokens":0,"slot_id":0,"task_id":103,"tid":"140543921174272","timestamp":1712585854,"truncated":false}
[GIN] 2024/04/08 - 14:17:34 | 200 |  209.165526ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":115,"tid":"140543921174272","timestamp":1712585962}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":3,"n_past_se":0,"n_prompt_tokens_processed":143,"slot_id":0,"task_id":115,"tid":"140543921174272","timestamp":1712585962}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":3,"slot_id":0,"task_id":115,"tid":"140543921174272","timestamp":1712585962}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     119.97 ms /   143 tokens (    0.84 ms per token,  1191.94 tokens per second)","n_prompt_tokens_processed":143,"n_tokens_second":1191.944787116994,"slot_id":0,"t_prompt_processing":119.972,"t_token":0.8389650349650349,"task_id":115,"tid":"140543921174272","timestamp":1712585964}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2316.91 ms /   139 runs   (   16.67 ms per token,    59.99 tokens per second)","n_decoded":139,"n_tokens_second":59.99359492842634,"slot_id":0,"t_token":16.66844604316547,"t_token_generation":2316.914,"task_id":115,"tid":"140543921174272","timestamp":1712585964}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2436.89 ms","slot_id":0,"t_prompt_processing":119.972,"t_token_generation":2316.914,"t_total":2436.8860000000004,"task_id":115,"tid":"140543921174272","timestamp":1712585964}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":285,"n_ctx":2048,"n_past":284,"n_system_tokens":0,"slot_id":0,"task_id":115,"tid":"140543921174272","timestamp":1712585964,"truncated":false}
[GIN] 2024/04/08 - 14:19:24 | 200 |  2.439171727s |       127.0.0.1 | POST     "/api/chat"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":257,"tid":"140543921174272","timestamp":1712586003}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":284,"n_past_se":0,"n_prompt_tokens_processed":39,"slot_id":0,"task_id":257,"tid":"140543921174272","timestamp":1712586003}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":284,"slot_id":0,"task_id":257,"tid":"140543921174272","timestamp":1712586003}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      74.83 ms /    39 tokens (    1.92 ms per token,   521.17 tokens per second)","n_prompt_tokens_processed":39,"n_tokens_second":521.1674150096215,"slot_id":0,"t_prompt_processing":74.832,"t_token":1.9187692307692306,"task_id":257,"tid":"140543921174272","timestamp":1712586008}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    4787.03 ms /   281 runs   (   17.04 ms per token,    58.70 tokens per second)","n_decoded":281,"n_tokens_second":58.700267451787965,"slot_id":0,"t_token":17.035697508896796,"t_token_generation":4787.031,"task_id":257,"tid":"140543921174272","timestamp":1712586008}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    4861.86 ms","slot_id":0,"t_prompt_processing":74.832,"t_token_generation":4787.031,"t_total":4861.863,"task_id":257,"tid":"140543921174272","timestamp":1712586008}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":604,"n_ctx":2048,"n_past":603,"n_system_tokens":0,"slot_id":0,"task_id":257,"tid":"140543921174272","timestamp":1712586008,"truncated":false}
[GIN] 2024/04/08 - 14:20:08 | 200 |  4.864725099s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/04/08 - 14:20:47 | 200 |      48.143µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 14:20:59 | 200 |      39.453µs |       127.0.0.1 | GET      "/api/version"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":541,"tid":"140543921174272","timestamp":1712586094}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1378,"slot_id":0,"task_id":541,"tid":"140543921174272","timestamp":1712586094}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":541,"tid":"140543921174272","timestamp":1712586094}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     946.59 ms /  1378 tokens (    0.69 ms per token,  1455.75 tokens per second)","n_prompt_tokens_processed":1378,"n_tokens_second":1455.7455466652016,"slot_id":0,"t_prompt_processing":946.594,"t_token":0.686933236574746,"task_id":541,"tid":"140543921174272","timestamp":1712586096}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1195.11 ms /    69 runs   (   17.32 ms per token,    57.74 tokens per second)","n_decoded":69,"n_tokens_second":57.73512630186434,"slot_id":0,"t_token":17.320478260869567,"t_token_generation":1195.113,"task_id":541,"tid":"140543921174272","timestamp":1712586096}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2141.71 ms","slot_id":0,"t_prompt_processing":946.594,"t_token_generation":1195.113,"t_total":2141.7070000000003,"task_id":541,"tid":"140543921174272","timestamp":1712586096}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1447,"n_ctx":2048,"n_past":1446,"n_system_tokens":0,"slot_id":0,"task_id":541,"tid":"140543921174272","timestamp":1712586096,"truncated":true}
[GIN] 2024/04/08 - 14:21:36 | 200 |  2.151138278s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2024/04/08 - 14:21:54 | 200 |      37.053µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2024/04/08 - 14:22:11 | 200 |      39.282µs |       127.0.0.1 | GET      "/api/version"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":613,"tid":"140543921174272","timestamp":1712586150}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1589,"slot_id":0,"task_id":613,"tid":"140543921174272","timestamp":1712586150}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":613,"tid":"140543921174272","timestamp":1712586150}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =    1134.72 ms /  1589 tokens (    0.71 ms per token,  1400.35 tokens per second)","n_prompt_tokens_processed":1589,"n_tokens_second":1400.3479278552027,"slot_id":0,"t_prompt_processing":1134.718,"t_token":0.7141082441787288,"task_id":613,"tid":"140543921174272","timestamp":1712586152}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     262.38 ms /    16 runs   (   16.40 ms per token,    60.98 tokens per second)","n_decoded":16,"n_tokens_second":60.98025764158854,"slot_id":0,"t_token":16.39875,"t_token_generation":262.38,"task_id":613,"tid":"140543921174272","timestamp":1712586152}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1397.10 ms","slot_id":0,"t_prompt_processing":1134.718,"t_token_generation":262.38,"t_total":1397.098,"task_id":613,"tid":"140543921174272","timestamp":1712586152}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1605,"n_ctx":2048,"n_past":1604,"n_system_tokens":0,"slot_id":0,"task_id":613,"tid":"140543921174272","timestamp":1712586152,"truncated":true}
[GIN] 2024/04/08 - 14:22:32 | 200 |  1.412692444s |       127.0.0.1 | POST     "/api/chat"
time=2024-04-08T14:22:32.308Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-08T14:22:33.516Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:22:33.516Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T14:22:33.516Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:22:33.516Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.9"
time=2024-04-08T14:22:33.516Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-08T14:22:33.517Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1293111853/runners/cuda_v11/libext_server.so"
time=2024-04-08T14:22:33.517Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 23 key-value pairs and 322 tensors from /teamspace/studios/this_studio/.ollama/models/blobs/sha256-8a9611e7bca168be635d39d21927d2b8e7e8ea0b5d0998b7d5980daf1f8d4205 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = command-r
llama_model_loader: - kv   1:                               general.name str              = c4ai-command-r-v01
llama_model_loader: - kv   2:                      command-r.block_count u32              = 40
llama_model_loader: - kv   3:                   command-r.context_length u32              = 131072
llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192
llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528
llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64
llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64
llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500
llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,256000]  = ["<PAD>", "<UNK>", "<CLS>", "<SEP>", ...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,253333]  = ["Ġ Ġ", "Ġ t", "e r", "i n", "Ġ a...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 5
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 255001
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   41 tensors
llama_model_loader: - type q4_0:  280 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 1008/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = command-r
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 253333
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 64
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 8192
llm_load_print_meta: n_embd_v_gqa     = 8192
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 6.2e-02
llm_load_print_meta: n_ff             = 22528
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = none
llm_load_print_meta: freq_base_train  = 8000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 35B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 34.98 B
llm_load_print_meta: model size       = 18.83 GiB (4.62 BPW) 
llm_load_print_meta: general.name     = c4ai-command-r-v01
llm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'
llm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'
llm_load_print_meta: PAD token        = 0 '<PAD>'
llm_load_print_meta: LF token         = 136 'Ä'
llm_load_tensors: ggml ctx size =    0.25 MiB
[GIN] 2024/04/08 - 14:23:11 | 200 |   12.188211ms |       127.0.0.1 | GET      "/api/version"
llm_load_tensors: offloading 36 repeating layers to GPU
llm_load_tensors: offloaded 36/41 layers to GPU
llm_load_tensors:        CPU buffer size = 19281.91 MiB
llm_load_tensors:      CUDA0 buffer size = 15877.12 MiB
.......................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 8000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =  2304.00 MiB
llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =   516.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  2172.62 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    36.00 MiB
llama_new_context_with_model: graph nodes  = 1245
llama_new_context_with_model: graph splits = 44
loading library /tmp/ollama1293111853/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"140544147715840","timestamp":1712586245}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"140544147715840","timestamp":1712586245}
time=2024-04-08T14:24:05.610Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"140543921174272","timestamp":1712586245}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712586245}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1772,"slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712586245}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"140543921174272","timestamp":1712586245}
